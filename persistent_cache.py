"""
æ°¸ç¶šã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«

GitHub Actionsã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥æ©Ÿèƒ½ã¨é€£æºã—ã¦ã€æ ªä¾¡ãƒ‡ãƒ¼ã‚¿ã‚’æ°¸ç¶šåŒ–ã—ã¾ã™ã€‚
å·®åˆ†æ›´æ–°ã«å¯¾å¿œã—ã€éŠ˜æŸ„ã‚³ãƒ¼ãƒ‰ã®ã¿ã‚’ã‚­ãƒ¼ã¨ã—ã¦åŠ¹ç‡çš„ã«ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã—ã¾ã™ã€‚
"""

import os
import pickle
import logging
from pathlib import Path
from datetime import datetime, timedelta
from typing import Optional, Tuple
import pandas as pd

logger = logging.getLogger(__name__)


class PersistentPriceCache:
    """
    ãƒ•ã‚¡ã‚¤ãƒ«ãƒ™ãƒ¼ã‚¹ã®æ°¸ç¶šæ ªä¾¡ãƒ‡ãƒ¼ã‚¿ã‚­ãƒ£ãƒƒã‚·ãƒ¥ï¼ˆå·®åˆ†æ›´æ–°å¯¾å¿œï¼‰
    
    GitHub Actionsã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥æ©Ÿèƒ½ã¨é€£æºã—ã¦ã€
    æ ªä¾¡ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã‚·ã‚¹ãƒ†ãƒ ã«ä¿å­˜ãƒ»èª­ã¿è¾¼ã¿ã—ã¾ã™ã€‚
    
    ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚­ãƒ¼ã¯éŠ˜æŸ„ã‚³ãƒ¼ãƒ‰ã®ã¿ã¨ã—ã€å·®åˆ†æ›´æ–°ã«å¯¾å¿œã—ã¾ã™ã€‚
    """
    
    def __init__(self, cache_dir: str = "~/.cache/stock_prices"):
        """
        Args:
            cache_dir: ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ãƒ‘ã‚¹
        """
        self.cache_dir = Path(cache_dir).expanduser()
        self.cache_dir.mkdir(parents=True, exist_ok=True)
        
        self.hits = 0
        self.misses = 0
        
        logger.info(f"æ°¸ç¶šã‚­ãƒ£ãƒƒã‚·ãƒ¥åˆæœŸåŒ–: {self.cache_dir}")
    
    def _get_cache_path(self, stock_code: str) -> Path:
        """
        ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ã‚’å–å¾—
        
        Args:
            stock_code: éŠ˜æŸ„ã‚³ãƒ¼ãƒ‰
        
        Returns:
            ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
        """
        return self.cache_dir / f"{stock_code}.pkl"
    
    def _load_cache_data(self, cache_path: Path) -> Optional[Tuple[pd.DataFrame, str]]:
        """
        ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã¨æœ€çµ‚æ›´æ–°æ—¥ã‚’èª­ã¿è¾¼ã‚€
        
        Args:
            cache_path: ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
        
        Returns:
            (DataFrame, æœ€çµ‚æ›´æ–°æ—¥YYYYMMDD)ã®ã‚¿ãƒ—ãƒ«ã€å¤±æ•—æ™‚ã¯None
        """
        if not cache_path.exists():
            return None
        
        try:
            with open(cache_path, 'rb') as f:
                data = pickle.load(f)
            
            # æ–°å½¢å¼: {'df': DataFrame, 'last_date': 'YYYYMMDD'}
            if isinstance(data, dict) and 'df' in data and 'last_date' in data:
                return data['df'], data['last_date']
            
            # æ—§å½¢å¼ï¼ˆäº’æ›æ€§ã®ãŸã‚ï¼‰: DataFrameã®ã¿
            if isinstance(data, pd.DataFrame):
                # DataFrameã‹ã‚‰æœ€çµ‚æ—¥ä»˜ã‚’å–å¾—
                if 'Date' in data.columns and len(data) > 0:
                    last_date = pd.to_datetime(data['Date'].iloc[-1]).strftime('%Y%m%d')
                    return data, last_date
                else:
                    return None
            
            return None
        
        except Exception as e:
            logger.warning(f"ã‚­ãƒ£ãƒƒã‚·ãƒ¥èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼ [{cache_path.name}]: {e}")
            return None
    
    def _save_cache_data(self, cache_path: Path, df: pd.DataFrame, last_date: str) -> bool:
        """
        ãƒ‡ãƒ¼ã‚¿ã¨æœ€çµ‚æ›´æ–°æ—¥ã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
        
        Args:
            cache_path: ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
            df: ä¿å­˜ã™ã‚‹DataFrame
            last_date: æœ€çµ‚æ›´æ–°æ—¥ï¼ˆYYYYMMDDï¼‰
        
        Returns:
            æˆåŠŸã—ãŸã‚‰True
        """
        try:
            data = {
                'df': df,
                'last_date': last_date
            }
            
            with open(cache_path, 'wb') as f:
                pickle.dump(data, f)
            
            logger.debug(f"ã‚­ãƒ£ãƒƒã‚·ãƒ¥ä¿å­˜: {cache_path.name} (æœ€çµ‚æ—¥: {last_date})")
            return True
        
        except Exception as e:
            logger.warning(f"ã‚­ãƒ£ãƒƒã‚·ãƒ¥ä¿å­˜ã‚¨ãƒ©ãƒ¼ [{cache_path.name}]: {e}")
            return False
    
    async def get(
        self,
        stock_code: str,
        start_date: str,
        end_date: str,
        max_age_days: int = 30
    ) -> Optional[pd.DataFrame]:
        """
        ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ï¼ˆå¿…è¦ãªæœŸé–“ã®ã¿è¿”ã™ï¼‰
        
        Args:
            stock_code: éŠ˜æŸ„ã‚³ãƒ¼ãƒ‰
            start_date: é–‹å§‹æ—¥ï¼ˆYYYYMMDDï¼‰
            end_date: çµ‚äº†æ—¥ï¼ˆYYYYMMDDï¼‰
            max_age_days: æœ€å¤§æœ‰åŠ¹æ—¥æ•°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ30æ—¥ï¼‰
        
        Returns:
            ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã•ã‚ŒãŸDataFrameï¼ˆæŒ‡å®šæœŸé–“ã®ã¿ï¼‰ã€ãªã‘ã‚Œã°None
        """
        cache_path = self._get_cache_path(stock_code)
        
        # ãƒ‡ãƒãƒƒã‚°ãƒ­ã‚°: å–å¾—é–‹å§‹
        logger.debug(f"ğŸ” ã‚­ãƒ£ãƒƒã‚·ãƒ¥å–å¾—é–‹å§‹: {stock_code}")
        logger.debug(f"  start_date: {start_date}, end_date: {end_date}")
        logger.debug(f"  ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ•ã‚¡ã‚¤ãƒ«å­˜åœ¨: {cache_path.exists()}")
        
        result = self._load_cache_data(cache_path)
        
        if result is None:
            self.misses += 1
            logger.debug(f"  âŒ ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ•ã‚¡ã‚¤ãƒ«ãªã— or èª­ã¿è¾¼ã¿å¤±æ•—")
            return None
        
        df, last_date = result
        
        # ãƒ‡ãƒãƒƒã‚°ãƒ­ã‚°: ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ‡ãƒ¼ã‚¿æƒ…å ±
        logger.debug(f"  âœ… ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿æˆåŠŸ: {len(df)}è¡Œ, æœ€çµ‚æ—¥: {last_date}")
        if 'Date' in df.columns and len(df) > 0:
            logger.debug(f"  Dateç¯„å›²: {df['Date'].min()} ~ {df['Date'].max()}")
        
        # æœ€çµ‚æ›´æ–°æ—¥ãŒå¤ã™ãã‚‹å ´åˆã¯ç„¡åŠ¹
        try:
            last_update = datetime.strptime(last_date, '%Y%m%d')
            age = datetime.now() - last_update
            
            if age.days > max_age_days:
                logger.debug(f"ã‚­ãƒ£ãƒƒã‚·ãƒ¥æœŸé™åˆ‡ã‚Œ: {stock_code} (æœ€çµ‚æ›´æ–°: {last_date}, {age.days}æ—¥å‰)")
                self.misses += 1
                return None
        except Exception as e:
            logger.warning(f"æ—¥ä»˜è§£æã‚¨ãƒ©ãƒ¼ [{stock_code}]: {e}")
            self.misses += 1
            return None
        
        # å¿…è¦ãªæœŸé–“ã®ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º
        try:
            if 'Date' in df.columns:
                df['Date'] = pd.to_datetime(df['Date'])
                start_dt = pd.to_datetime(start_date, format='%Y%m%d')
                end_dt = pd.to_datetime(end_date, format='%Y%m%d')
                
                filtered_df = df[(df['Date'] >= start_dt) & (df['Date'] <= end_dt)].copy()
                
                # ãƒ‡ãƒãƒƒã‚°ãƒ­ã‚°: ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°çµæœ
                logger.debug(f"  ç¬¬1ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼: {len(filtered_df)}è¡Œ (start_dt <= Date <= end_dt)")
                
                # å¿…è¦ãªæœŸé–“ã®ãƒ‡ãƒ¼ã‚¿ãŒååˆ†ã«ã‚ã‚‹ã‹ç¢ºèª
                if len(filtered_df) > 0:
                    self.hits += 1
                    logger.debug(f"  âœ… ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ’ãƒƒãƒˆ: {stock_code} ({len(filtered_df)}è¡Œ)")
                    return filtered_df
                
                # end_dt ãŒæœ€æ–°ãƒ‡ãƒ¼ã‚¿ã‚ˆã‚Šæ–°ã—ã„å ´åˆã€start_dtä»¥é™ã®ã™ã¹ã¦ã®ãƒ‡ãƒ¼ã‚¿ã‚’è¿”ã™
                # ï¼ˆåœŸæ—¥å®Ÿè¡Œæ™‚ã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒŸã‚¹ãƒãƒƒãƒå¯¾ç­–ï¼‰
                filtered_df = df[df['Date'] >= start_dt].copy()
                
                # ãƒ‡ãƒãƒƒã‚°ãƒ­ã‚°: ç¬¬2ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼çµæœ
                logger.debug(f"  ç¬¬2ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼: {len(filtered_df)}è¡Œ (Date >= start_dt)")
                
                if len(filtered_df) > 0:
                    self.hits += 1
                    logger.debug(f"  âœ… ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ’ãƒƒãƒˆï¼ˆéƒ¨åˆ†ï¼‰: {stock_code} ({len(filtered_df)}è¡Œ, end_dtè¶…é)")
                    return filtered_df
                else:
                    logger.debug(f"  âŒ ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«å¿…è¦ãªæœŸé–“ã®ãƒ‡ãƒ¼ã‚¿ãªã—: {stock_code}")
                    logger.debug(f"     start_dt: {start_dt}, ã‚­ãƒ£ãƒƒã‚·ãƒ¥æœ€å¤æ—¥: {df['Date'].min() if len(df) > 0 else 'N/A'}")
                    self.misses += 1
                    return None
            else:
                self.misses += 1
                return None
        
        except Exception as e:
            logger.warning(f"ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã‚¨ãƒ©ãƒ¼ [{stock_code}]: {e}")
            self.misses += 1
            return None
    
    async def set(
        self,
        stock_code: str,
        start_date: str,
        end_date: str,
        df: pd.DataFrame
    ) -> bool:
        """
        ãƒ‡ãƒ¼ã‚¿ã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜ï¼ˆå·®åˆ†æ›´æ–°å¯¾å¿œï¼‰
        
        æ—¢å­˜ã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãŒã‚ã‚‹å ´åˆã¯ã€æ–°ã—ã„ãƒ‡ãƒ¼ã‚¿ã¨ãƒãƒ¼ã‚¸ã—ã¾ã™ã€‚
        
        Args:
            stock_code: éŠ˜æŸ„ã‚³ãƒ¼ãƒ‰
            start_date: é–‹å§‹æ—¥ï¼ˆYYYYMMDDï¼‰
            end_date: çµ‚äº†æ—¥ï¼ˆYYYYMMDDï¼‰
            df: ä¿å­˜ã™ã‚‹DataFrame
        
        Returns:
            æˆåŠŸã—ãŸã‚‰True
        """
        if df is None or len(df) == 0:
            return False
        
        cache_path = self._get_cache_path(stock_code)
        
        # æ—¢å­˜ã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’èª­ã¿è¾¼ã‚€
        result = self._load_cache_data(cache_path)
        
        if result is not None:
            existing_df, _ = result
            
            # æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã¨æ–°ã—ã„ãƒ‡ãƒ¼ã‚¿ã‚’ãƒãƒ¼ã‚¸
            try:
                # Dateåˆ—ã‚’ datetime å‹ã«å¤‰æ›
                existing_df['Date'] = pd.to_datetime(existing_df['Date'])
                df['Date'] = pd.to_datetime(df['Date'])
                
                # é‡è¤‡ã‚’å‰Šé™¤ã—ã¦ãƒãƒ¼ã‚¸
                merged_df = pd.concat([existing_df, df]).drop_duplicates(subset=['Date'], keep='last')
                merged_df = merged_df.sort_values('Date').reset_index(drop=True)
                
                # æœ€çµ‚æ—¥ä»˜ã‚’å–å¾—
                last_date = merged_df['Date'].iloc[-1].strftime('%Y%m%d')
                
                logger.debug(f"ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒãƒ¼ã‚¸: {stock_code} (æ—¢å­˜: {len(existing_df)}è¡Œ, æ–°è¦: {len(df)}è¡Œ, åˆè¨ˆ: {len(merged_df)}è¡Œ)")
                
                return self._save_cache_data(cache_path, merged_df, last_date)
            
            except Exception as e:
                logger.warning(f"ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒãƒ¼ã‚¸ã‚¨ãƒ©ãƒ¼ [{stock_code}]: {e}")
                # ãƒãƒ¼ã‚¸å¤±æ•—æ™‚ã¯æ–°ã—ã„ãƒ‡ãƒ¼ã‚¿ã§ä¸Šæ›¸ã
        
        # æ–°è¦ä¿å­˜
        try:
            df['Date'] = pd.to_datetime(df['Date'])
            last_date = df['Date'].iloc[-1].strftime('%Y%m%d')
            return self._save_cache_data(cache_path, df, last_date)
        
        except Exception as e:
            logger.warning(f"ã‚­ãƒ£ãƒƒã‚·ãƒ¥ä¿å­˜ã‚¨ãƒ©ãƒ¼ [{stock_code}]: {e}")
            return False
    
    def get_stats(self) -> dict:
        """
        ã‚­ãƒ£ãƒƒã‚·ãƒ¥çµ±è¨ˆã‚’å–å¾—
        
        Returns:
            çµ±è¨ˆæƒ…å ±ã®è¾æ›¸
        """
        total_files = len(list(self.cache_dir.glob("*.pkl")))
        total_size_mb = sum(
            f.stat().st_size for f in self.cache_dir.glob("*.pkl")
        ) / (1024 * 1024)
        
        total_requests = self.hits + self.misses
        hit_rate = (self.hits / total_requests * 100) if total_requests > 0 else 0
        
        return {
            "files": total_files,
            "size_mb": round(total_size_mb, 2),
            "hits": self.hits,
            "misses": self.misses,
            "hit_rate": round(hit_rate, 2)
        }
    
    def clear_old_cache(self, max_age_days: int = 30):
        """
        å¤ã„ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤
        
        Args:
            max_age_days: å‰Šé™¤ã™ã‚‹æœ€å¤§æ—¥æ•°
        """
        now = datetime.now()
        deleted_count = 0
        
        for cache_file in self.cache_dir.glob("*.pkl"):
            # ãƒ•ã‚¡ã‚¤ãƒ«ã®æœ€çµ‚æ›´æ–°æ—¥æ™‚ã‚’ãƒã‚§ãƒƒã‚¯
            mtime = datetime.fromtimestamp(cache_file.stat().st_mtime)
            age = now - mtime
            
            if age.days >= max_age_days:
                cache_file.unlink()
                deleted_count += 1
        
        if deleted_count > 0:
            logger.info(f"å¤ã„ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’å‰Šé™¤: {deleted_count}ãƒ•ã‚¡ã‚¤ãƒ«")
