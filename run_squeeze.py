#!/usr/bin/env python3
"""ã‚¹ã‚¯ã‚¤ãƒ¼ã‚ºï¼ˆä¾¡æ ¼åç¸®ï¼‰ã‚¹ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°å°‚ç”¨ã‚¹ã‚¯ãƒªãƒ—ãƒˆ"""

import asyncio
import sys
import os
import pandas as pd
from datetime import datetime, timedelta
from daily_data_collection import (
    StockScreener, 
    sample_stocks_balanced,
    logger,
    CONCURRENT_REQUESTS
)

async def main():
    """ã‚¹ã‚¯ã‚¤ãƒ¼ã‚ºã®ã¿ã‚’å®Ÿè¡Œ"""
    screener = StockScreener()
    
    try:
        # ä»®ã®å®Ÿè¡Œæ—¥ï¼ˆå¾Œã§æœ€æ–°å–å¼•æ—¥ã«æ›´æ–°ï¼‰
        target_date = datetime.now().strftime('%Y-%m-%d')
        logger.info("=" * 80)
        logger.info(f"ã‚¹ã‚¯ã‚¤ãƒ¼ã‚ºï¼ˆä¾¡æ ¼åç¸®ï¼‰ã‚¹ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°é–‹å§‹")
        logger.info("=" * 80)
        
        # å®Ÿè¡Œãƒˆãƒªã‚¬ãƒ¼ã‚’åˆ¤å®š
        trigger = os.environ.get('GITHUB_EVENT_NAME', 'unknown')
        is_manual = (trigger == 'workflow_dispatch')
        
        # å–¶æ¥­æ—¥ãƒã‚§ãƒƒã‚¯
        import aiohttp
        async with aiohttp.ClientSession() as session:
            is_trading = await screener.client.is_trading_day(session, target_date)
            
            if not is_trading:
                if is_manual:
                    logger.warning(f"âš ï¸  {target_date}ã¯ä¼‘æ—¥ã§ã™ãŒã€æ‰‹å‹•å®Ÿè¡Œã®ãŸã‚å‡¦ç†ã‚’ç¶šè¡Œã—ã¾ã™")
                else:
                    return
        
        logger.info(f"âœ… å®Ÿè¡Œæ—¥: {target_date}")
        logger.info("ğŸ“Š Supabaseæ¥ç¶šæˆåŠŸ")
        
        # éŠ˜æŸ„ä¸€è¦§å–å¾—
        stocks = await screener.get_stocks_list()
        
        if not stocks:
            logger.error("âŒ éŠ˜æŸ„ä¸€è¦§ã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ")
            sys.exit(1)
        
        logger.info(f"âœ… éŠ˜æŸ„ä¸€è¦§å–å¾—å®Œäº†: {len(stocks)}éŠ˜æŸ„")
        logger.info(f"åŒæ™‚å®Ÿè¡Œæ•°: {CONCURRENT_REQUESTS}")
        logger.info("=" * 80)
        
        sq_start = datetime.now()
        squeeze = await screener.process_stocks_batch(
            stocks, screener.screen_stock_squeeze, "ã‚¹ã‚¯ã‚¤ãƒ¼ã‚º"
        )
        sq_time = int((datetime.now() - sq_start).total_seconds() * 1000)
        logger.info(f"âœ… ã‚¹ã‚¯ã‚¤ãƒ¼ã‚ºæ¤œå‡º: {len(squeeze)}éŠ˜æŸ„ ({sq_time}ms)")
        
        # æœ€æ–°å–å¼•æ—¥ã‚’å–å¾—ï¼ˆæ¤œå‡ºã•ã‚ŒãŸéŠ˜æŸ„ã‹ã‚‰ï¼‰
        if squeeze:
            first_stock = squeeze[0]
            code = first_stock["code"]
            end_date = datetime.now()
            start_date = end_date - timedelta(days=10)
            start_str = start_date.strftime("%Y%m%d")
            end_str = end_date.strftime("%Y%m%d")
            
            import aiohttp
            async with aiohttp.ClientSession() as session:
                df = await screener.cache.get_or_fetch(
                    code, start_str, end_str,
                    screener.jq_client.get_prices_daily_quotes,
                    session, code, start_str, end_str
                )
                if df is not None and len(df) > 0:
                    latest_date = df.iloc[-1]['Date']
                    target_date = pd.to_datetime(latest_date).strftime('%Y-%m-%d')
                    logger.info(f"ğŸ“… æœ€æ–°å–å¼•æ—¥: {target_date}")
        
        # é–“å¼•ãå‡¦ç†
        squeeze_sampled = sample_stocks_balanced(squeeze, max_per_range=10)
        logger.info(f"ğŸ“Š é–“å¼•ãå¾Œ: {len(squeeze_sampled)}éŠ˜æŸ„")
        
        # Supabaseä¿å­˜
        screening_id = screener.sb_client.save_screening_result(
            "squeeze", target_date,
            len(squeeze), sq_time
        )
        if screening_id:
            # additional_dataã¨ã—ã¦JSONBå½¢å¼ã§ä¿å­˜
            stocks_with_additional_data = []
            for s in squeeze_sampled:
                stock_data = {
                    "code": s["code"],
                    "name": s["name"],
                    "price": s["price"],
                    "market": s["market"],
                    "volume": s.get("volume", 0),
                    "additional_data": {
                        "bbw": s.get("bbw"),
                        "deviation": s.get("deviation"),
                        "atr": s.get("atr"),
                        "days": s.get("days")
                    }
                }
                stocks_with_additional_data.append(stock_data)
            
            screener.sb_client.save_detected_stocks(screening_id, stocks_with_additional_data)
            logger.info(f"ğŸ’¾ Supabaseä¿å­˜å®Œäº† (screening_id: {screening_id})")
        
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥çµ±è¨ˆã‚’å‡ºåŠ›
        logger.info("=" * 80)
        logger.info("ãƒ¡ãƒ¢ãƒªã‚­ãƒ£ãƒƒã‚·ãƒ¥çµ±è¨ˆ:")
        screener.cache.log_stats()
        
        # æ°¸ç¶šã‚­ãƒ£ãƒƒã‚·ãƒ¥çµ±è¨ˆã‚’å‡ºåŠ›
        persistent_stats = screener.persistent_cache.get_stats()
        logger.info("\næ°¸ç¶šã‚­ãƒ£ãƒƒã‚·ãƒ¥çµ±è¨ˆ:")
        logger.info(f"  ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {persistent_stats['files']}ä»¶")
        logger.info(f"  åˆè¨ˆã‚µã‚¤ã‚º: {persistent_stats['size_mb']}MB")
        logger.info(f"  ãƒ’ãƒƒãƒˆæ•°: {persistent_stats['hits']}å›")
        logger.info(f"  ãƒŸã‚¹æ•°: {persistent_stats['misses']}å›")
        logger.info(f"  ãƒ’ãƒƒãƒˆç‡: {persistent_stats['hit_rate']}%")
        logger.info("=" * 80)
        
        logger.info("âœ… ã‚¹ã‚¯ã‚¤ãƒ¼ã‚ºã‚¹ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°å®Œäº†")
        logger.info("=" * 80)
        
    except Exception as e:
        logger.error(f"âŒ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}", exc_info=True)
        sys.exit(1)

if __name__ == "__main__":
    asyncio.run(main())
