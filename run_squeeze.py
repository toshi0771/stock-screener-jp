#!/usr/bin/env python3
"""ã‚¹ã‚¯ã‚¤ãƒ¼ã‚ºï¼ˆä¾¡æ ¼åç¸®ï¼‰ã‚¹ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°å°‚ç”¨ã‚¹ã‚¯ãƒªãƒ—ãƒˆ"""

import asyncio
import sys
import os
import pandas as pd
from datetime import datetime, timedelta
from daily_data_collection import (
    StockScreener, 
    sample_stocks_balanced,
    logger,
    CONCURRENT_REQUESTS
)

async def main():
    """ã‚¹ã‚¯ã‚¤ãƒ¼ã‚ºã®ã¿ã‚’å®Ÿè¡Œ"""
    screener = StockScreener()
    
    try:
        # ä»®ã®å®Ÿè¡Œæ—¥ï¼ˆå¾Œã§æœ€æ–°å–å¼•æ—¥ã«æ›´æ–°ï¼‰
        target_date = datetime.now().strftime('%Y-%m-%d')
        logger.info("=" * 80)
        logger.info(f"ã‚¹ã‚¯ã‚¤ãƒ¼ã‚ºï¼ˆä¾¡æ ¼åç¸®ï¼‰ã‚¹ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°é–‹å§‹")
        logger.info("=" * 80)
        
        # å®Ÿè¡Œãƒˆãƒªã‚¬ãƒ¼ã‚’åˆ¤å®š
        trigger = os.environ.get('GITHUB_EVENT_NAME', 'unknown')
        is_manual = (trigger == 'workflow_dispatch')
        
        # å–¶æ¥­æ—¥ãƒã‚§ãƒƒã‚¯
        import aiohttp
        async with aiohttp.ClientSession() as session:
            is_trading = await screener.client.is_trading_day(session, target_date)
            
            if not is_trading:
                if is_manual:
                    logger.warning(f"âš ï¸  {target_date}ã¯ä¼‘æ—¥ã§ã™ãŒã€æ‰‹å‹•å®Ÿè¡Œã®ãŸã‚å‡¦ç†ã‚’ç¶šè¡Œã—ã¾ã™")
                else:
                    return
        
        logger.info(f"âœ… å®Ÿè¡Œæ—¥: {target_date}")
        logger.info("ğŸ“Š Supabaseæ¥ç¶šæˆåŠŸ")
        
        # éŠ˜æŸ„ä¸€è¦§å–å¾—
        stocks = await screener.get_stocks_list()
        
        if not stocks:
            logger.error("âŒ éŠ˜æŸ„ä¸€è¦§ã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ")
            sys.exit(1)
        
        logger.info(f"âœ… éŠ˜æŸ„ä¸€è¦§å–å¾—å®Œäº†: {len(stocks)}éŠ˜æŸ„")
        
        # ğŸ”§ FIX: æœ€æ–°å–å¼•æ—¥ã‚’äº‹å‰ã«å–å¾—ã—ã¦ã‚­ãƒ£ãƒƒã‚·ãƒ¥ï¼ˆã‚¹ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°å‰ã«å®Ÿè¡Œï¼‰
        screener.latest_trading_date = await screener.get_latest_trading_date()
        logger.info(f"ğŸ“… æœ€æ–°å–å¼•æ—¥ï¼ˆã‚¹ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°ç”¨ï¼‰: {screener.latest_trading_date}")
        
        logger.info(f"åŒæ™‚å®Ÿè¡Œæ•°: {CONCURRENT_REQUESTS}")
        logger.info("=" * 80)
        
        sq_start = datetime.now()
        squeeze = await screener.process_stocks_batch(
            stocks, screener.screen_stock_squeeze, "ã‚¹ã‚¯ã‚¤ãƒ¼ã‚º"
        )
        sq_time = int((datetime.now() - sq_start).total_seconds() * 1000)
        logger.info(f"âœ… ã‚¹ã‚¯ã‚¤ãƒ¼ã‚ºæ¤œå‡º: {len(squeeze)}éŠ˜æŸ„ ({sq_time}ms)")
        
        # ã‚¹ã‚¯ã‚¤ãƒ¼ã‚ºæ¡ä»¶ã®çµ±è¨ˆæƒ…å ±ã‚’å‡ºåŠ›
        if hasattr(screener, 'squeeze_stats'):
            stats = screener.squeeze_stats
            logger.info("\n" + "="*80)
            logger.info("ğŸ“Š ã‚¹ã‚¯ã‚¤ãƒ¼ã‚ºæ¡ä»¶ãƒã‚§ãƒƒã‚¯çµæœ:")
            logger.info(f"  å…¨éŠ˜æŸ„æ•°: {stats['total']}")
            logger.info(f"  ãƒ‡ãƒ¼ã‚¿å–å¾—æˆåŠŸ: {stats['has_data']}éŠ˜æŸ„")
            logger.info(f"  BBWæ¡ä»¶ã§é™¤å¤–: {stats['bbw_failed']}éŠ˜æŸ„")
            logger.info(f"  ä¹–é›¢ç‡æ¡ä»¶ã§é™¤å¤–: {stats['deviation_failed']}éŠ˜æŸ„")
            logger.info(f"  ATRæ¡ä»¶ã§é™¤å¤–: {stats['atr_failed']}éŠ˜æŸ„")
            logger.info(f"  ç¶™ç¶šæ—¥æ•°ä¸è¶³ã§é™¤å¤–: {stats['duration_failed']}éŠ˜æŸ„")
            logger.info(f"  æœ€çµ‚æ¤œå‡ºæ•°: {stats['passed_all']}éŠ˜æŸ„")
            logger.info("="*80)
        
        # ğŸ”§ FIX: æ—¢ã«å–å¾—æ¸ˆã¿ãªã®ã§å†å–å¾—ä¸è¦ï¼ˆdatetimeã‚’æ–‡å­—åˆ—ã«å¤‰æ›ï¼‰
        target_date = screener.latest_trading_date.strftime('%Y-%m-%d')
        logger.info(f"ğŸ“… æœ€æ–°å–å¼•æ—¥ï¼ˆä¿å­˜ç”¨ï¼‰: {target_date}")
        
        # é–“å¼•ãå‡¦ç†
        squeeze_sampled = sample_stocks_balanced(squeeze, max_per_range=10)
        logger.info(f"ğŸ“Š é–“å¼•ãå¾Œ: {len(squeeze_sampled)}éŠ˜æŸ„")
        
        # Supabaseä¿å­˜
        screening_id = screener.sb_client.save_screening_result(
            "squeeze", target_date,
            len(squeeze), sq_time
        )
        if screening_id:
            # additional_dataã¨ã—ã¦JSONBå½¢å¼ã§ä¿å­˜
            stocks_with_additional_data = []
            for s in squeeze_sampled:
                stock_data = {
                    "code": s["code"],
                    "name": s["name"],
                    "price": s["price"],
                    "market": s["market"],
                    "volume": s.get("volume", 0),
                    "additional_data": {
                        "bbw": s.get("bbw"),
                        "deviation": s.get("deviation"),
                        "atr": s.get("atr"),
                        "days": s.get("days")
                    }
                }
                stocks_with_additional_data.append(stock_data)
            
            screener.sb_client.save_detected_stocks(screening_id, stocks_with_additional_data)
            logger.info(f"ğŸ’¾ Supabaseä¿å­˜å®Œäº† (screening_id: {screening_id})")
        
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥çµ±è¨ˆã‚’å‡ºåŠ›
        logger.info("=" * 80)
        logger.info("ãƒ¡ãƒ¢ãƒªã‚­ãƒ£ãƒƒã‚·ãƒ¥çµ±è¨ˆ:")
        screener.cache.log_stats()
        
        # æ°¸ç¶šã‚­ãƒ£ãƒƒã‚·ãƒ¥çµ±è¨ˆã‚’å‡ºåŠ›
        persistent_stats = screener.persistent_cache.get_stats()
        logger.info("\næ°¸ç¶šã‚­ãƒ£ãƒƒã‚·ãƒ¥çµ±è¨ˆ:")
        logger.info(f"  ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {persistent_stats['files']}ä»¶")
        logger.info(f"  åˆè¨ˆã‚µã‚¤ã‚º: {persistent_stats['size_mb']}MB")
        logger.info(f"  ãƒ’ãƒƒãƒˆæ•°: {persistent_stats['hits']}å›")
        logger.info(f"  ãƒŸã‚¹æ•°: {persistent_stats['misses']}å›")
        logger.info(f"  ãƒ’ãƒƒãƒˆç‡: {persistent_stats['hit_rate']}%")
        logger.info("=" * 80)
        
        logger.info("âœ… ã‚¹ã‚¯ã‚¤ãƒ¼ã‚ºã‚¹ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°å®Œäº†")
        logger.info("=" * 80)
        
    except Exception as e:
        logger.error(f"âŒ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}", exc_info=True)
        sys.exit(1)

if __name__ == "__main__":
    asyncio.run(main())
